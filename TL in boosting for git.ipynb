{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1fd83cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras import layers\n",
    "from pandas import ExcelWriter\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.pyplot import figure\n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "import os\n",
    "from keras import regularizers\n",
    "from numpy import array, hstack, vstack\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "from numpy import asarray\n",
    "from pandas import read_csv\n",
    "from pandas import DataFrame\n",
    "from pandas import concat\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from xgboost import XGBRegressor\n",
    "from matplotlib import pyplot\n",
    " \n",
    "\n",
    "os.chdir(\"C:/Users/VIP13/Статья 2/Котировки 6\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "259c6d65",
   "metadata": {},
   "outputs": [],
   "source": [
    "def series_to_supervised(data, n_in=1, n_out=1, dropnan=True):\n",
    "\tn_vars = 1 if type(data) is list else data.shape[1]\n",
    "\tdf = DataFrame(data)\n",
    "\tcols = list()\n",
    "\t# input sequence (t-n, ... t-1)\n",
    "\tfor i in range(n_in, 0, -1):\n",
    "\t\tcols.append(df.shift(i))\n",
    "\t# forecast sequence (t, t+1, ... t+n)\n",
    "\tfor i in range(0, n_out):\n",
    "\t\tcols.append(df.shift(-i))\n",
    "\t# put it all together\n",
    "\tagg = concat(cols, axis=1)\n",
    "\t# drop rows with NaN values\n",
    "\tif dropnan:\n",
    "\t\tagg.dropna(inplace=True)\n",
    "\treturn agg.values\n",
    "\n",
    "# split a univariate dataset into train/test sets\n",
    "def train_test_split(data, n_test):\n",
    "\treturn data[:-n_test, :], data[-n_test:, :]\n",
    " \n",
    "# fit an xgboost model and make a one step prediction\n",
    "def xgboost_forecast(train,eta, gamma, max_depth):\n",
    "\t# transform list into array\n",
    "\ttrain = asarray(train)\n",
    "\t# split into input and output columns\n",
    "\ttrainX, trainy = train[:, :-1], train[:, -1]\n",
    "\t# fit model\n",
    "\tmodel = XGBRegressor(objective='reg:squarederror', n_estimators=100, eta=eta, gamma=gamma, max_depth=max_depth)\n",
    "\tmodel.fit(trainX, trainy)\n",
    "\t# make a one-step prediction\n",
    "# \tyhat = model.predict(asarray([testX]))\n",
    "\treturn model\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abf86cd1",
   "metadata": {},
   "source": [
    "# Functions for tbbss/tssbb and tbs/tsb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55847d07",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import random\n",
    "\n",
    "# tbbss/tssbb\n",
    "def transf_learn(frame_learn, frame_transf, n_in, len_wind, step_smooth, eta, gamma, max_depth):\n",
    "    iteratation_5=int((len(frame_learn)-len_wind)/step_smooth)+1\n",
    "#     iteratation_5=15\n",
    "\n",
    "    # prepared a dataframe to put the results of the prediction in it\n",
    "    columnss=[]\n",
    "    columnss.append(frame_learn.columns[0])\n",
    "    for k in range(0,len(frame_transf.columns)):\n",
    "        columnss.append(str(frame_transf.columns[k]))\n",
    "\n",
    "    prog=pd.DataFrame(columns=columnss)\n",
    "    orig=pd.DataFrame(columns=columnss)\n",
    "\n",
    "    for i in range(0, iteratation_5):\n",
    "    #They merged two frames together in sequence. first they beat them into samples, then they merged them\n",
    "        frame_learn_use=pd.DataFrame(frame_learn.iloc[0+step_smooth*i:len_wind+step_smooth*i])\n",
    "        frame_learn_use=series_to_supervised(frame_learn_use, n_in=n_in)\n",
    "    \n",
    "        for j in range(0,len(frame_transf.columns)):\n",
    "            fr2=pd.DataFrame(frame_transf[frame_transf.columns[j]].iloc[0+step_smooth*i:len_wind+step_smooth*i])\n",
    "            fr2=series_to_supervised(fr2, n_in=n_in)\n",
    "            frame_learn_use=vstack([frame_learn_use, fr2])\n",
    "\n",
    "        model = xgboost_forecast(frame_learn_use, eta=eta, gamma=gamma, max_depth=max_depth)\n",
    "\n",
    "        # forecasting\n",
    "        tmp_prog=[]\n",
    "        tmp_orig=[]\n",
    "\n",
    "        series=DataFrame(frame_learn.iloc[0+step_smooth*i:len_wind+1+step_smooth*i])\n",
    "        values = series.values\n",
    "        # print(series)\n",
    "        data_test = series_to_supervised(values, n_in=n_in)[-1]\n",
    "        testX, testy = data_test[:-1], data_test[-1]\n",
    "        yhat = model.predict(asarray([testX]))[0]\n",
    "        tmp_prog.append(yhat)\n",
    "        tmp_orig.append(testy)\n",
    "\n",
    "        for j in range(0,len(frame_transf.columns)):\n",
    "            series=DataFrame(frame_transf.iloc[0+step_smooth*i:len_wind+1+step_smooth*i,j:j+1])\n",
    "            values = series.values\n",
    "            data_test = series_to_supervised(values, n_in=n_in)[-1]\n",
    "            testX, testy = data_test[:-1],data_test[-1]\n",
    "            yhat = model.predict(asarray([testX]))[0]\n",
    "            tmp_prog.append(yhat)\n",
    "            tmp_orig.append(testy)\n",
    "        prog.loc[i]=np.exp(tmp_prog)\n",
    "        orig.loc[i]=np.exp(tmp_orig)\n",
    "    return orig, prog\n",
    "\n",
    "\n",
    "# tbs/tsb\n",
    "def one_learn(frame_learn, frame_forecast, n_in, len_wind, step_smooth, eta, gamma, max_depth):\n",
    "    iteratation_5=int((len(frame_learn)-len_wind)/step_smooth)+1\n",
    "#     iteratation_5=15\n",
    "\n",
    "    # frame for results of forecasting\n",
    "    columnss=[]\n",
    "    columnss.append(frame_learn.columns[0])\n",
    "    for k in range(0,len(frame_forecast.columns)):\n",
    "        columnss.append(str(frame_forecast.columns[k]))\n",
    "\n",
    "    prog=pd.DataFrame(columns=columnss)\n",
    "    orig=pd.DataFrame(columns=columnss)\n",
    "    for i in range(0, iteratation_5):\n",
    "    #обучили\n",
    "        frame_learn_use=pd.DataFrame(frame_learn.iloc[0+step_smooth*i:len_wind+step_smooth*i])\n",
    "        frame_learn_use=series_to_supervised(frame_learn_use, n_in=n_in)\n",
    "        model = xgboost_forecast(frame_learn_use, eta=eta, gamma=gamma, max_depth=max_depth)\n",
    "\n",
    "        # forecasting\n",
    "        tmp_prog=[]\n",
    "        tmp_orig=[]\n",
    "\n",
    "        series=DataFrame(frame_learn.iloc[0+step_smooth*i:len_wind+1+step_smooth*i])\n",
    "        values = series.values\n",
    "        # print(series)\n",
    "        data_test = series_to_supervised(values, n_in=n_in)[-1]\n",
    "        testX, testy = data_test[:-1], data_test[-1]\n",
    "        yhat = model.predict(asarray([testX]))[0]\n",
    "        tmp_prog.append(yhat)\n",
    "        tmp_orig.append(testy)\n",
    "\n",
    "        for j in range(0,len(frame_forecast.columns)):\n",
    "            series=DataFrame(frame_forecast.iloc[0+step_smooth*i:len_wind+1+step_smooth*i,j:j+1])\n",
    "            values = series.values\n",
    "            data_test = series_to_supervised(values, n_in=n_in)[-1]\n",
    "            testX, testy = data_test[:-1],data_test[-1]\n",
    "            yhat = model.predict(asarray([testX]))[0]\n",
    "            tmp_prog.append(yhat)\n",
    "            tmp_orig.append(testy)\n",
    "        prog.loc[i]=np.exp(tmp_prog)\n",
    "        orig.loc[i]=np.exp(tmp_orig)\n",
    "    return orig, prog"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcab3d60",
   "metadata": {},
   "source": [
    "# Frame for results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c90e11e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "names=[]\n",
    "for eta in np.arange(0.1,1,0.1):\n",
    "    for gamma in np.arange(0.1,1,0.1):\n",
    "        for max_depth in np.arange(1,11,1):\n",
    "#             for reg_lambda in np.arange(0,1,0.1):\n",
    "#                 for reg_alpha in np.arange(0,1,0.1):\n",
    "#                     for subsample in np.arange(0,1,0.1):\n",
    "#                         for colsample_bytree in np.arange(0,1,0.1):\n",
    "            name='boost'+'_'+str(eta)+'_'+str(gamma)+'_'+str(max_depth)\n",
    "            names.append(name)\n",
    "\n",
    "all_frame_btc=pd.DataFrame(columns=names)    \n",
    "all_frame_snp=pd.DataFrame(columns=names)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6c451f9",
   "metadata": {},
   "source": [
    "# Data for TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7b190f",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_3=pd.read_csv('all_snp_btc_rv_rr.csv', delimiter=';')\n",
    "frame=frame_3.iloc[:,1:].copy() #на входе подаются квадраты волатильностей, поэтому сначала из них берем корни, а потом логарифмируем\n",
    "frame=frame.loc[:, [c for c in frame.columns if (frame[c]>0).all()]]\n",
    "frame=np.log(np.sqrt(frame))\n",
    "ind=[i for i in range(0,len(frame))]\n",
    "frame.index=ind\n",
    "\n",
    "frame_learns=pd.DataFrame()\n",
    "frame_learns['rv_shk_btc']=frame['rv_shk_btc']\n",
    "# frame_learn['rv_shk_btc']=frame['rv_shk_btc']\n",
    "\n",
    "frame_transfs=pd.DataFrame()\n",
    "frame_transfs['rv_shk_snp']=frame['rv_shk_snp']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6550dc70",
   "metadata": {},
   "source": [
    "# tbbss/tssbb TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14c691aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "frame_3=pd.read_csv('all_snp_btc_rv_rr.csv', delimiter=';')\n",
    "frame=frame_3.iloc[:,1:].copy() #на входе подаются квадраты волатильностей, поэтому сначала из них берем корни, а потом логарифмируем\n",
    "frame=frame.loc[:, [c for c in frame.columns if (frame[c]>0).all()]]\n",
    "frame=np.log(np.sqrt(frame))\n",
    "ind=[i for i in range(0,len(frame))]\n",
    "frame.index=ind\n",
    "\n",
    "frame_learns=pd.DataFrame()\n",
    "frame_learns['rv_shk_btc']=frame['rv_shk_btc']\n",
    "# frame_learn['rv_shk_btc']=frame['rv_shk_btc']\n",
    "\n",
    "frame_transfs=pd.DataFrame()\n",
    "frame_transfs['rv_shk_snp']=frame['rv_shk_snp']\n",
    "\n",
    "all_frame_btc1=all_frame_btc.copy()\n",
    "all_frame_snp1=all_frame_snp.copy()\n",
    "\n",
    "for eta in np.arange(0.1,1,0.1):\n",
    "    for gamma in np.arange(0.1,1,0.1):\n",
    "        for max_depth in np.arange(1,11,1):\n",
    "            ll=ll+1\n",
    "            name='boost'+'_'+str(eta)+'_'+str(gamma)+'_'+str(max_depth)\n",
    "#           tbbss\n",
    "            prog=transf_learn(frame_learn=frame_learns, frame_transf=frame_transfs, \n",
    "                n_in=5, len_wind=399, step_smooth=5, eta=eta, gamma=gamma, max_depth=max_depth)\n",
    "            all_frame_btc1[name]=prog[1]['rv_shk_btc']\n",
    "            all_frame_snp1[name]=prog[1]['rv_shk_snp']\n",
    "            print(ll)\n",
    "            \n",
    "            \n",
    "\n",
    "all_frame_btc1['orig_data']=prog[0]['rv_shk_btc']\n",
    "all_frame_snp1['orig_data']=prog[0]['rv_shk_snp']\n",
    "all_frame_btc1.to_csv('tbbss_boosting_btc.csv')\n",
    "all_frame_snp1.to_csv('tbbss_boosting_snp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc82be5a",
   "metadata": {},
   "source": [
    "# tbs TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ffca83",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frame_btc2=all_frame_btc.copy()\n",
    "all_frame_snp2=all_frame_snp.copy()\n",
    "\n",
    "for eta in np.arange(0.1,1,0.1):\n",
    "    for gamma in np.arange(0.1,1,0.1):\n",
    "        for max_depth in np.arange(1,11,1):\n",
    "            ll=ll+1\n",
    "            name='boost'+'_'+str(eta)+'_'+str(gamma)+'_'+str(max_depth)\n",
    "#           tbs\n",
    "            prog=one_learn(frame_learn=frame_learns, frame_forecast=frame_transfs, \n",
    "                n_in=5, len_wind=399, step_smooth=5, eta=eta, gamma=gamma, max_depth=max_depth)\n",
    "            all_frame_btc2[name]=prog[1]['rv_shk_btc']\n",
    "            all_frame_snp2[name]=prog[1]['rv_shk_snp']\n",
    "            print(ll)\n",
    "            \n",
    "            \n",
    "\n",
    "all_frame_btc2['orig_data']=prog[0]['rv_shk_btc']\n",
    "all_frame_snp2['orig_data']=prog[0]['rv_shk_snp']\n",
    "all_frame_btc2.to_csv('tbs_boosting_btc.csv')\n",
    "all_frame_snp2.to_csv('tbs_boosting_snp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28b2a09b",
   "metadata": {},
   "source": [
    "# tsb TL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b04e4e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_frame_btc3=all_frame_btc.copy()\n",
    "all_frame_snp3=all_frame_snp.copy()\n",
    "\n",
    "for eta in np.arange(0.1,1,0.1):\n",
    "    for gamma in np.arange(0.1,1,0.1):\n",
    "        for max_depth in np.arange(1,11,1):\n",
    "            ll=ll+1\n",
    "            name='boost'+'_'+str(eta)+'_'+str(gamma)+'_'+str(max_depth)\n",
    "#            tsb\n",
    "            prog=one_learn(frame_learn=frame_transfs, frame_forecast=frame_learns, \n",
    "                n_in=5, len_wind=399, step_smooth=5, eta=eta, gamma=gamma, max_depth=max_depth)\n",
    "            all_frame_btc3[name]=prog[1]['rv_shk_btc']\n",
    "            all_frame_snp3[name]=prog[1]['rv_shk_snp']\n",
    "            print(ll)\n",
    "            \n",
    "            \n",
    "\n",
    "all_frame_btc3['orig_data']=prog[0]['rv_shk_btc']\n",
    "all_frame_snp3['orig_data']=prog[0]['rv_shk_snp']\n",
    "all_frame_btc3.to_csv('tsb_boosting_btc.csv')\n",
    "all_frame_snp3.to_csv('tsb_boosting_snp.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56a644da",
   "metadata": {},
   "source": [
    "# Analyzing the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883ed01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "# frame=pd.read_csv('tsb_boosting_btc.csv', delimiter=',',decimal='.')\n",
    "# frame=pd.read_csv('tsb_boosting_snp.csv', delimiter=',',decimal='.')\n",
    "# frame=pd.read_csv('tbs_boosting_btc.csv', delimiter=',',decimal='.')\n",
    "# frame=pd.read_csv('tbs_boosting_snp.csv', delimiter=',',decimal='.')\n",
    "# frame=pd.read_csv('tbbss_boosting_btc.csv', delimiter=',',decimal='.')\n",
    "frame=pd.read_csv('tbbss_boosting_snp.csv', delimiter=',',decimal='.')\n",
    "orig=frame['orig_data']\n",
    "\n",
    "frae=pd.DataFrame(orig)\n",
    "\n",
    "for i in range(1, len(frame.columns)-1):\n",
    "    prognoz=frame[frame.columns[i]]\n",
    "#     pri=[]\n",
    "#     for j in prognoz:\n",
    "#         pri.append(float(j[1:-1]))\n",
    "    frae[frame.columns[i]]=prognoz  \n",
    "        \n",
    "mape=[]\n",
    "mape.append(0)\n",
    "c=frae[frae.columns[0]]\n",
    "for i in range(1,len(frae.columns)):\n",
    "    cc=frae[frae.columns[i]]\n",
    "    mape.append((100*abs(c-cc)/c).mean())\n",
    "# print(mape)\n",
    "fraes=frae.transpose()\n",
    "fraes['mape']=mape\n",
    "frae_sort=fraes.sort_values(by='mape')\n",
    "frae_sort=frae_sort.transpose()\n",
    "# print(frae_sort)\n",
    "print(frae_sort.iloc[-1,1:6])\n",
    "frame_pr=pd.DataFrame(frae[frae.columns[0]])\n",
    "accuracy=[]\n",
    "accuracy.append(0)\n",
    "for i in range(1,len(frae.columns)):\n",
    "    frame_pr['prog']=frae[frae.columns[i]]\n",
    "    fr=frame_pr.iloc[1:,0:].copy()\n",
    "    ind=[i for i in range(0,len(frame_pr)-1)]\n",
    "    fr.index=ind\n",
    "    frr=frame_pr.iloc[:-1,0:].copy()\n",
    "    frr.index=ind\n",
    "    frame_diff=fr-frr\n",
    "    frame_diff[frame_diff < 0] = 0\n",
    "    frame_diff[frame_diff > 0] = 1\n",
    "\n",
    "    for j in range(1,len(frame_diff.columns)):\n",
    "        accuracy.append(accuracy_score(frame_diff.iloc[:,0], frame_diff.iloc[:,j]))\n",
    "# print(accuracy)\n",
    "fraes['accuracy']=accuracy\n",
    "\n",
    "frae_sort_1=fraes.sort_values(by='accuracy', ascending=False)\n",
    "frae_sort_1=frae_sort_1.transpose()\n",
    "print(frae_sort_1.iloc[-1,0:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9721f0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "name_fr=pd.DataFrame(frae_sort.iloc[-1,1:6]).transpose().columns[0]\n",
    "print(frae_sort_1[name_fr].iloc[-2:])\n",
    "\n",
    "name_fr_1=pd.DataFrame(frae_sort_1.iloc[-1,0:6]).transpose().columns[0]\n",
    "print(frae_sort[name_fr_1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
